{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69d94db4",
   "metadata": {},
   "source": [
    "$$\\text{Trabajo Final de Comunicaciones Digitales - Parte N° 1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8031bcfe",
   "metadata": {},
   "source": [
    "$\\text{Diseño del Codificador - Decodificador }$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9a4e4c",
   "metadata": {},
   "source": [
    "# Marco Teórico\n",
    "\n",
    "## Codificador\n",
    "\n",
    "El **codificador** es el dispositivo que toma el mensaje y produce la palabra código. El mensaje consiste en una secuencia de bits de información que debe ser transformada para su transmisión eficiente.\n",
    "\n",
    "Si queremos comunicar una secuencia de $k$ bits, existen $2^k$ secuencias distintas y cada una debería ser mapeada a una secuencia de símbolos distinta. Esta relación se expresa matemáticamente como:\n",
    "\n",
    "$$2^k \\leq m^n$$\n",
    "\n",
    "donde $m$ es el tamaño del alfabeto de símbolos y $n$ es la longitud de la secuencia de símbolos.\n",
    "\n",
    "### Codificación Binaria Directa\n",
    "\n",
    "La base matemática para lo que llamamos **codificación binaria directa** consiste en agrupar $k$ bits y interpretarlos como un entero entre $0$ y $2^{k}-1$, el cual se mapea a un símbolo dentro de un alfabeto de tamaño $m = 2^k$.\n",
    "\n",
    "La fórmula que describe esta transformación es:\n",
    "\n",
    "$$\\text{Símbolo} = \\sum_{i=0}^{SF-1} b_i \\cdot 2^i$$\n",
    "\n",
    "donde:\n",
    "- $b_i$ es el $i$-ésimo bit del grupo\n",
    "- $SF$ es el Spreading Factor (Factor de Expansión)\n",
    "\n",
    "## Decodificador\n",
    "\n",
    "El **decodificador** es el dispositivo responsable de inferir el mensaje original a partir de la señal recibida. En el contexto del canal AWGN (Ruido Blanco Gaussiano Aditivo), el decodificador implementa una estrategia de **Máxima Verosimilitud** (Maximum Likelihood).\n",
    "\n",
    "### Decodificador de Máxima Verosimilitud\n",
    "\n",
    "Para un decodificador ML en el canal AWGN en tiempo discreto, se elige una de las secuencias de salida $\\mathbf{x}$ que maximiza la función de verosimilitud. La métrica de decisión se expresa como:\n",
    "\n",
    "$$\\langle \\mathbf{c}, \\mathbf{y} \\rangle - \\frac{|\\mathbf{c}|^2}{2}$$\n",
    "\n",
    "donde:\n",
    "- $\\mathbf{c}$ es la palabra código candidata\n",
    "- $\\mathbf{y}$ es la señal recibida\n",
    "- $\\langle \\mathbf{c}, \\mathbf{y} \\rangle$ es el producto interno entre ambos vectores\n",
    "- $|\\mathbf{c}|^2$ es la energía de la palabra código"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c166d6a5",
   "metadata": {},
   "source": [
    "$\\text{Desarrollo}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fad5af",
   "metadata": {},
   "source": [
    "En el paper, el proceso de codificación en la modulación por desplazamiento de frecuencia mediante chirps se describe matemáticamente de manera precisa y puede descomponerse en tres etapas fundamentales:\n",
    "## Parte 1: Modulacion de chirp por desplazamiento de frecuencia\n",
    "\n",
    "Etapa 1: Mapeo de bits a símbolo decimal\n",
    "\n",
    "El proceso comienza con un vector $w(nT_s)$ compuesto por una cantidad determinada de bits. Esta cantidad se denomina **Spreading Factor** (SF). El Spreading Factor es el número de bits por símbolo y se calcula como:\n",
    "\n",
    "$$SF = \\log_2(M)$$\n",
    "\n",
    "donde $M$ es la cardinalidad de la modulación (la cantidad de símbolos distintos). Por ejemplo, si $SF = 5$, hay $2^5 = 32$ símbolos posibles.\n",
    "\n",
    "Una vez obtenido el vector $w(nT_s)$, se utiliza la fórmula planteada en el paper para sumar los valores ponderados de los bits del vector $w(nT_s)$ y así generar el valor decimal entero $s(nT_s)$. Este número se utiliza para decidir con qué frecuencia inicial el chirp inicia la transmisión.\n",
    "\n",
    "### Parámetros de transmisión\n",
    "\n",
    "Supongamos que el ancho de banda del canal que usamos para la transmisión es $B$ y que transmitimos una muestra cada $T$, con:\n",
    "\n",
    "$$T = \\frac{1}{B}$$\n",
    "\n",
    "Un símbolo $s(nT_s)$ es enviado a la entrada del modulador cada:\n",
    "\n",
    "$$T_s = 2^{SF} \\cdot T$$\n",
    "\n",
    "El símbolo $s(nT_s)$ es un número real que se forma usando un vector $w(nT_s)$ de dígitos binarios SF, con SF como un parámetro entero llamado **Factor de Expansión**, el cual normalmente toma valores en $\\{7, 8, 9, 10, 11, 12\\}$.\n",
    "\n",
    "El SF (Spreading Factor) es el número de bits que se agrupan en cada símbolo. Cada número representa un entero entre $0$ y $2^{SF} - 1$ y representa cuánto se \"esparce\" la señal en el tiempo, determinando la cantidad de símbolos que se pueden transmitir.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5eaf08c",
   "metadata": {},
   "source": [
    "## Ecuación 1: Codificación de bits a símbolo\n",
    "\n",
    "El proceso de codificación transforma una secuencia de bits en símbolos que pueden ser transmitidos a través del canal. \n",
    "\n",
    "La codificación binaria directa agrupa $k$ bits consecutivos y los interpreta como un número entero entre $0$ y $2^k - 1$. Este número entero se mapea directamente a un símbolo dentro de un alfabeto de tamaño $m = 2^k$. \n",
    "\n",
    "La fórmula matemática que describe esta transformación es:\n",
    "\n",
    "$$s(nT_s) = \\sum_{h=0}^{SF-1} w_h(nT_s) \\cdot 2^h$$\n",
    "\n",
    "donde:\n",
    "- $s(nT_s) \\in {0,...,2^SF - 1}$ es el simbolo codificado \n",
    "- $w_h(nT_s) \\in {0,1}$ son los bits del bloque actual, ordenados desde el menos significativo (LSB).\n",
    "- $SF$ es el Spreading Factor (Factor de Expansión), que define la cantidad de bits por símbolo.\n",
    "- $h$ es el índice del bit dentro del bloque\n",
    "\n",
    "Entonces: \n",
    "    Esta operación convierte secuenci sbinarias en números enteros para su modulacíon, permitiendo la reperesentacíon \n",
    "    eficiente de la informacíon.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669e5b08",
   "metadata": {},
   "source": [
    "## Parte 2: Detección Óptima de Señales FSCM en Canales AWGN \n",
    "\n",
    "Como la señales tienen la misma energía y se supone sincronización perfecta en tiempp y frecuencia, así como símbolos igualmente probables, el receptor óptimo para señales FSCM en un canal AWGN puede derivarse directamente de la teoría clásica de detección de señales. \n",
    "\n",
    "Entonces, la fundamentación teórica del receptor óptimo es: \n",
    "\n",
    "### 1. Modelo del canal AWGN:\n",
    "$$r = s + w$$ \n",
    "\n",
    "donde: \n",
    "- r: señal recibida (discreta). \n",
    "- s: chirp transmitido. \n",
    "- w: ruido blanco gaussiano. \n",
    "\n",
    "### 2. Dectector óptimo (MAP/ML):\n",
    "\n",
    "\n",
    "### La señal recibida: \n",
    "$$r(nT_s + kT) = c(nT_s + kT) + w(nT_s + kT)$$ \n",
    "\n",
    "donde: \n",
    "- $r(nT_s + kT)$ es la muestra de la señal recibida. \n",
    "- $c(nT_s + kT)$ es la señal transmitida. \n",
    "- $w(nT_s + kT)$ es ruido blanco gaussiano de media cero. \n",
    "\n",
    "El detector óptimo en un canal AWGN elige el símbolo $\\hat{s}$ que maximiza la correlación entre la señal recibida r y cada posible chirp $c_q:$\n",
    "\n",
    " $$\\hat{s} = \\arg\\max_q \\left| \\sum_k r[k] \\cdot c_q^*[k] \\right|^2$$\n",
    "\n",
    "Las formas de onda chirp definidas para la modulación FSCM poseen la propiedad de ortoganalidad: \n",
    "\n",
    "$\\langle c_q, c_{q'} \\rangle = 0 \\quad \\text{si } q \\neq q'$\n",
    "\n",
    "Esta propiedad asegura que la proyección de la señal recibida sibre una forma de onda incorrecta será nula (idelamente), y máxima cuando la proyección se realiza sobre el chirp correspondiente al símbolo transmitido.\n",
    "La ortogonalidad es clave para permitir la detección ediciente mediante transformada rápida de Fourier (FFT)\n",
    "\n",
    "Implementacion eficiente: \n",
    "Para esto se multiplica por un down - chirp y se aplica una FFT. El índice del pico de la FFT da el símbolo recibido y la complejidad se reduce de $O(M^2)$ a $O(M log M),$ con $M = 2^SF$\n",
    "\n",
    "### Proceso óptimo de demodulación \n",
    "El demulador óptimo consiste en proyectar la señal recibida sobre todas las posibles formas $c(nT_s + kT)$ y selecciona aquella con la máxima correlación: \n",
    "$$\\hat{s}(nT_s) = \\arg\\max_q \\left| \\langle r(nT_s + kT), c(nT_s + kT)|_{s(nT_s)=q} \\rangle \\right|^2$$ \n",
    "\n",
    "1. Implementación computacionalmente eficiente \n",
    "Evitamos calcular todas las proyecciones explícitamente y usar la FFT \n",
    "pasos: \n",
    "a. Multiplicación por un down- chirp\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f468d502",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b5fcab1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "869cb87f",
   "metadata": {},
   "source": [
    "## Parte 2: Detección Óptima de Señales FSCM en Canales AWGN \n",
    "\n",
    "Como las señales tienen la misma energía y se supone sincronización perfecta en tiempo y frecuencia, así como símbolos igualmente probables, el receptor óptimo para señales FSCM en un canal AWGN puede derivarse directamente de la teoría clásica de detección de señales. \n",
    "\n",
    "Entonces, la fundamentación teórica del receptor óptimo es: \n",
    "\n",
    "### 1. Modelo del canal AWGN:rt{2^{SF}}} \\cdot e^{j2\\pi[(s(nT_s)+k) \\bmod 2^{SF}]\\frac{kT}{B}} \\quad (2)$$\n",
    "\n",
    "La señal recibida se modela como:donde:\n",
    "_s + kT)$ es la señal chirp modulada en el tiempo $nT_s + kT$\n",
    "$$r(nT_s + kT) = c(nT_s + kT) + w(nT_s + kT) \\quad (10)$$l factor de normalización de energía (asegura energía unitaria para cada símbolo)\n",
    "o codificado (de la Ecuación 1)\n",
    "donde: dentro del símbolo ($k = 0, 1, ..., 2^{SF}-1$)\n",
    "- $r(nT_s + kT)$ es la muestra de la señal recibida- $T$ es el período de muestreo\n",
    "- $c(nT_s + kT)$ es la señal chirp transmitidanal\n",
    "- $w(nT_s + kT)$ es ruido blanco gaussiano de media cero- $\\bmod 2^{SF}$ indica la operación módulo $2^{SF}$\n",
    "\n",
    "El ruido $w(nT_s + kT)$ tiene las siguientes características:ncipales:\n",
    "- Varianza: $\\sigma_w^2(nT_s + kT) = \\sigma_w^2$ (independiente de $(nT_s + kT)$)\n",
    "- Media cero1. **Chirp complejo**: La señal tiene frecuencia linealmente creciente dentro de cada símbolo\n",
    "- Estadísticamente independiente entre muestrasecuencia instantánea**: Depende de $(s(nT_s)+k) \\bmod 2^{SF}$, que representa un corrimiento de frecuencia específico para cada símbolo\n",
    " chirps ortogonales entre sí, facilitando la decodificación\n",
    "### 2. Detector óptimo (MAP/ML):\\sqrt{2^{SF}}}$ mantiene la energía constante por símbolo\n",
    "\n",
    "El **demodulador óptimo** consiste en proyectar la señal recibida $r(nT_s + kT)$ sobre las diferentes señales $c(nT_s + kT)|_{s(nT_s)=q}$ para $q = 0, ..., 2^{SF}-1$ y elegir la señal $c(nT_s + kT)|_{s(nT_s)=l}$ tal que el módulo cuadrado de la proyección sea máximo como la mejor estimación de la señal transmitida.\n",
    "\n",
    "Este proceso proporciona la mejor estimación $\\hat{s}(nT_s) = l$ de la señal transmitida $s(nT_s)$.**Ecuación 3 (forma simplifca:**)\n",
    "\n",
    "Matemáticamente, el detector óptimo elige el símbolo $\\hat{s}$ que maximiza la correlación:\n",
    "\n",
    "$$\\hat{s} = \\arg\\max_q \\left| \\sum_k r[k] \\cdot c_q^*[k] \\right|^2$$\n",
    "t \\frac{[(s(nT_s)+k) \\bmod 2^{SF}] \\cdot k}{2^{SF}}} \\quad (3)$$\n",
    "### 3. Propiedad de ortogonalidad:\n",
    "\n",
    "Las formas de onda chirp definidas para la modulación FSCM poseen la propiedad de ortogonalidad: \n",
    "1. Definiciíon del tiempo discreto:  \n",
    "$$\\langle c_q, c_{q'} \\rangle = 0 \\quad \\text{si } q \\neq q'$$\n",
    "\n",
    "Esta propiedad asegura que la proyección de la señal recibida sobre una forma de onda incorrecta será nula (idealmente), y máxima cuando la proyección se realiza sobre el chirp correspondiente al símbolo transmitido. \n",
    "ot T + kT$$\n",
    "La ortogonalidad es clave para permitir la detección eficiente mediante transformada rápida de Fourier (FFT).\n",
    "\n",
    "### 4. Implementación eficiente:\n",
    "reciente, donde la frecuencia instantánea varía linealmente con k, y \n",
    "Para esto se multiplica por un down-chirp y se aplica una FFT. El índice del pico de la FFT da el símbolo recibido y la complejidad se reduce de $O(M^2)$ a $O(M \\log M)$, con $M = 2^{SF}$.\n",
    "{[(s+k) \\bmod 2^{SF}] \\cdot k}{2^{SF}}$$\n",
    "### 5. Proceso óptimo de demodulación:\n",
    "\n",
    "El demodulador óptimo consiste en proyectar la señal recibida sobre todas las posibles formas $c(nT_s + kT)$ y selecciona aquella con la máxima correlación: 3. Fase acumulada y señal compleja \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "c. **Detección de pico:** El índice del máximo de la FFT corresponde al símbolo transmitidob. **Aplicación de FFT:** Se calcula la transformada rápida de Fourier del resultadoa. **Multiplicación por un down-chirp:** Se multiplica la señal recibida por el chirp de referencia conjugadoEvitamos calcular todas las proyecciones explícitamente usando la FFT en los siguientes pasos:#### Implementación computacionalmente eficiente:$$\\hat{s}(nT_s) = \\arg\\max_q \\left| \\langle r(nT_s + kT), c(nT_s + kT)|_{s(nT_s)=q} \\rangle \\right|^2$$ $$e^{j2\\pi \\cdot \\frac{[(s+k) \\bmod 2^{SF}] \\cdot k}{2^{SF}}}$$\n",
    "\n",
    "Representa una señal compleja cuya fase cambia con el tiempo (índice k).Esa fase varía en funcíon del símbolo transmitido s y de la muestra actual k. \n",
    "Como resultado, se genera una onda chirp (señal cuya frecuencia aumenta linealmente con k ). Esto se logra porque la fase del exponente crece de forma cuadrática con k, algo característico de un chirp lineal. \n",
    "El desplazamiento s hace que cada símbolo comience su chirp en una frecuencia distinta, lo que permite codificar la información.\n",
    "\n",
    "4. Normalizacíon de energía \n",
    "\n",
    "El factor $\\frac{1}{\\sqrt{2^{SF}}}$ normaliza la señal para que tenga energía unitaria. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33e127e",
   "metadata": {},
   "source": [
    "$\\text{Implementación en python}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9652cd32",
   "metadata": {},
   "source": [
    "<text>La función bits_to_symbols: Agrupa un arreglo plano de bits en bloques de longitud igual al Spreading Factor (SF) y \n",
    "\n",
    "convierte cada bloque en un número decimal. Este paso simula lo que en el paper se describe como la generación del símbolo\n",
    " \n",
    "𝑠(𝑛𝑇𝑠) a partir del vector de bits 𝑤(𝑛𝑇𝑠)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e43616cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def bits_to_symbols(bit_array, SF):\n",
    "    \"\"\"Agrupa los bits en bloques de SF y los convierte en símbolos enteros.\"\"\"\n",
    "    bit_array = np.array(bit_array).reshape(-1, SF)\n",
    "    powers = 2 ** np.arange(SF)[::-1]\n",
    "    symbols = bit_array.dot(powers)\n",
    "    return symbols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962eec88",
   "metadata": {},
   "source": [
    "<text>La funcion symbols to bits: Invierte el proceso anterior: toma una lista de símbolos y los convierte en sus \n",
    "correspondientes representaciones binarias de SF bits. Esto emula la decodificación, recuperando el vector 𝑤(𝑛𝑇𝑠) desde el número 𝑠(𝑛𝑇𝑠)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e4d0721",
   "metadata": {},
   "outputs": [],
   "source": [
    "def symbols_to_bits(symbols, SF):\n",
    "    \"\"\"Convierte los símbolos de vuelta a su representación binaria.\"\"\"\n",
    "    bits = ((symbols[:, None] & (1 << np.arange(SF)[::-1])) > 0).astype(int)\n",
    "    return bits.reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db2cc5e",
   "metadata": {},
   "source": [
    "<text>  La funcion simulate_encoder_decoder: Es el núcleo de la simulación. \n",
    "\n",
    "1~ Genera bits aleatorios con una distribución uniforme entre 0 y 1 usando np.random.randint.\n",
    "\n",
    "2~ Codifica: convierte esos bits en símbolos utilizando bits_to_symbols.\n",
    "\n",
    "3~ Simula la transmisión: en este primer modelo, la transmisión es perfecta. Es decir, los símbolos recibidos son iguales a los transmitidos (sin errores de canal, por ahora).\n",
    "\n",
    "4~ Decodifica: recupera los bits originales desde los símbolos usando symbols_to_bits.\n",
    "\n",
    "5~ Calcula el BER: compara los bits transmitidos vs. los recibidos para calcular la tasa de error de bit (Bit Error Rate).\n",
    "\n",
    "6~ Imprime resultados: muestra los primeros 64 bits transmitidos y decodificados, junto con el BER final y estadísticas generales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb738c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bits transmitidos (primeros 64):   [0 1 0 0 0 0 1 1 0 0 1 1 0 1 0 0 1 1 0 0 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 0\n",
      " 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 1 0]\n",
      "Bits decodificados (primeros 64): [0 1 0 0 0 0 1 1 0 0 1 1 0 1 0 0 1 1 0 0 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 0\n",
      " 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 1 0]\n",
      "\n",
      "Total de bits transmitidos: 7000\n",
      "Errores totales: 0\n",
      "BER (Bit Error Rate): 0.000000\n"
     ]
    }
   ],
   "source": [
    "def simulate_encoder_decoder(SF, total_bits):\n",
    "    \"\"\"Genera bits aleatorios, codifica, decodifica y calcula el BER.\"\"\"\n",
    "    assert total_bits % SF == 0, \"El número total de bits debe ser múltiplo de SF\"\n",
    "\n",
    "    # Generar bits aleatorios con distribución uniforme\n",
    "    tx_bits = np.random.randint(0, 2, total_bits)\n",
    "\n",
    "    # Codificación\n",
    "    tx_symbols = bits_to_symbols(tx_bits, SF)\n",
    "\n",
    "    # Transmisión simulada perfecta (sin ruido)\n",
    "    rx_symbols = tx_symbols.copy()  # En canal real, se podría agregar ruido o errores\n",
    "\n",
    "    # Decodificación\n",
    "    rx_bits = symbols_to_bits(rx_symbols, SF)\n",
    "\n",
    "    # Cálculo de BER\n",
    "    bit_errors = np.sum(tx_bits != rx_bits)\n",
    "    ber = bit_errors / total_bits\n",
    "\n",
    "    # Resultados\n",
    "    print(\"Bits transmitidos (primeros 64):  \", tx_bits[:64])\n",
    "    print(\"Bits decodificados (primeros 64):\", rx_bits[:64])\n",
    "    print(f\"\\nTotal de bits transmitidos: {total_bits}\")\n",
    "    print(f\"Errores totales: {bit_errors}\")\n",
    "    print(f\"BER (Bit Error Rate): {ber:.6f}\")\n",
    "\n",
    "# Parámetros de simulación\n",
    "SF = 7  # Spreading Factor\n",
    "num_bits = SF * 1000  # Enviar 1000 símbolos\n",
    "\n",
    "simulate_encoder_decoder(SF, num_bits)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
